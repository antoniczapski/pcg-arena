# PCG Arena (Stage 0) — Local Pairwise Rating for Mario PCG Generators

## What this repository is

This repository implements **PCG Arena — Stage 0**: a **local-only**, end-to-end prototype of a platform that lets a human player **play two Mario levels**, choose which one is better, and updates a **leaderboard of generators** based on those pairwise preferences.

Here we build the **arena infrastructure** that can evaluate *any* levels created via Procedural Content Generator by collecting human preference data.

### Why Stage 0 matters

Before building a web service or trying to attract users, Stage 0 validates the core loop:

1. **Pick two levels** (each attributed to a generator).
2. Player plays both inside a **Java client**.
3. Player submits a **preference vote** (Left wins / Right wins / Tie / Skip).
4. Backend updates ratings and persists everything so restarting does **not** lose history.

This is directly inspired by the general idea used in the **Mario AI Championship “Level Generation Track”**, where humans play content generated by different generators and choose what they prefer. 

---

## Project goals (Stage 0)

### Primary goal

**Prove the core preference-based evaluation loop works locally and reproducibly.**

### Secondary goals

* Make the system **extensible**: adding a new generator levels should be “drop in new level set + metadata”.
* Keep the design close to what a hosted MVP would need later (clean API boundaries, persistent storage, audit logs).
* Collect clean data suitable for later research (pairwise comparisons + metadata + session events).

---

## Stage 0 scope (what’s in / what’s out)

### In scope

* Local backend in a **Docker container**: API + rating logic.
* Local database persisted **outside** the container (Docker volume or bind mount).
* Local Java client executable that:

  * fetches a “battle” (two candidate levels),
  * renders and runs gameplay,
  * collects a vote + optional tags,
  * submits results.

### Out of scope (explicitly not Stage 0)

* Public hosting, accounts, OAuth, anti-cheat at scale.
* Browser-playable Mario.
* Uploading arbitrary generator code from users (sandboxing).
* “Continuous generation” (Stage 0 assumes a fixed pool of levels on disk).

---

## Architecture (Stage 0)

Stage 0 is three components with **hard boundaries** so each can be developed independently:

1. **Client (Java game executable)**
2. **Backend (Docker container)**
3. **Database (persistent local storage outside the container)**

### 1) Java Client responsibilities

* Treat the backend as a black box.
* Ask for a battle: “Give me two levels to compare.”
* Run both levels with identical UX structure (so comparisons are fair).
* Collect:

  * **vote**: LEFT/RIGHT/TIE/SKIP
  * optional: completion, deaths, time, subjective tags
* Submit vote to backend and show updated rating snapshot (or at least confirmation).

### 2) Backend responsibilities

* Own all business logic:

  * selecting battles,
  * tracking sessions,
  * validating vote payloads,
  * updating ratings,
  * writing everything to the DB.
* Serve a stable API contract so the Java client doesn’t need to know internals.
* Provide admin/debug endpoints locally (optional but useful).

### 3) Database responsibilities

* Persist:

  * generators and their metadata,
  * levels and attribution,
  * battles served,
  * votes received,
  * rating history (or at least current rating + match count).
* Must survive:

  * backend container restart,
  * backend image rebuild.

---

## Repository layout

### Current structure (Stage 0)

```
pcg-arena/
  README.md
  docker-compose.yml       # ✅ One-command setup (docker compose up)
  .gitignore               # ✅ Excludes db/local/, client-java/dist/

  backend/                 # ✅ Dockerized Python/FastAPI backend
    spec.md                # ✅ Backend implementation documentation
    Dockerfile             # ✅ Container build instructions
    requirements.txt       # ✅ Python dependencies (FastAPI, Uvicorn)
    config/
      settings.env         # ✅ Environment variable defaults
    src/
      __init__.py          # ✅ Package metadata
      config.py            # ✅ Configuration loader
      main.py              # ✅ FastAPI app + endpoints
      db/
        __init__.py        # ✅ Module exports
        connection.py      # ✅ SQLite connection management
        migrations.py      # ✅ Migration runner
        seed.py            # ✅ Seed importer + validation
    tests/                 # ⏳ Placeholder
    openapi/               # ⏳ Placeholder
    scripts/               # ⏳ Placeholder

  db/
    spec.md                # ✅ Database specification
    migrations/            # ✅ SQL migration scripts
      001_init.sql         # ✅ Core tables (7 tables)
      002_indexes.sql      # ✅ Performance indexes
    seed/                  # ✅ Initial dataset
      generators.json      # ✅ 3 generators (hopper, genetic, notch)
      levels/              # ✅ 30 level files (10 per generator)
        genetic/
        hopper/
        notch/
    local/                 # ✅ Runtime storage (gitignored)
      arena.sqlite         # ✅ SQLite database (created on first run)

  client-java/             # ✅ Java client (validation prototype)
    spec.md                # ✅ Client specification
    build.gradle           # ✅ Gradle build configuration
    src/
      main/java/arena/
        api/               # ✅ HTTP client + models
        config/            # ✅ Configuration loader
        game/              # ✅ Mario game engine (ported from Framework)
        ui/                # ✅ Swing UI (gameplay panels, voting)
        util/              # ✅ JSON, logging, time utilities
      main/resources/
        img/               # ✅ Mario sprite sheets
        logback.xml        # ✅ Logging configuration
    logs/                  # Runtime logs (gitignored)
    build/                 # Build artifacts (gitignored)

  shared/                  # ⏳ Placeholder
    schemas/
    level-format/

  docs/
    stage0-spec.md         # ✅ Detailed technical specification
    stage1-spec.md         # ✅ Stage 1 deployment specification
    future-notes.md        # ✅ Future roadmap notes
  
  Mario-AI-Framework-PCG/  # ✅ Source of game engine and levels
    src/engine/            # ✅ Mario physics (ported to client-java)
    levels/                # ✅ 9000+ generated levels (not imported yet)
    img/                   # ✅ Sprite sheets (copied to client-java)
```

**Legend:**  
✅ Implemented | ⏳ Placeholder / Not yet implemented

### Notes

* `docker-compose.yml` orchestrates backend + volume mounts
* `db/local/` and `client-java/dist/` are gitignored
* Backend serves HTTP API on `localhost:8080`
* Database persists across container restarts

---

## Stage 0 data model (conceptual)

Minimum entities:

* **Generator**

  * `generator_id`, `name`, `version`, `description`, `tags`, `documentation-url`
* **Level**

  * `level_id`, `generator_id`, `content_blob`, `content_format`, `hash`
* **Battle**

  * `battle_id`, `left_level_id`, `right_level_id`, `served_at`, `session_id`
* **Vote**

  * `vote_id`, `battle_id`, `result` (LEFT/RIGHT/TIE/SKIP), `created_at`
  * optional telemetry: `left_tags`, `right_tags`, `telemetry_json`
* **Rating**

  * `generator_id`, `rating_value`, `rating_deviation` (optional), `games_played`

Stage 0 should also store **immutable logs** (battle served → vote received) so you can debug issues and later publish datasets.

---

## Rating system (Stage 0 recommendation)

Because the platform is about **pairwise human preference**, you want a rating that:

* updates online,
* supports uncertainty / cold-start,
* is easy to explain.

### Minimal implementation: Elo (pairwise winners)

* Treat each generator as a “player”.
* Each vote is a “match” between two generators.
* Outcome: win/loss/tie/skip.
* Update Elo after each match.

This gives you the “ELO-like generator leaderboard” you want with extremely low complexity.

---

## Level format (Stage 0)

Stage 0 only needs a format that:

* the Java client can load quickly,
* the backend can treat as opaque bytes + metadata,
* supports a fixed set of tile types.

We will use **ASCII grid / tilemap** (easy, transparent)

It will always be a rectangle of variable width (up to 250) by 16 (LEVEL_HEIGHT) lines, consisting of ASCII characters. E.g. `-` denotes air, `X` ground, `t` pipe or `o` coin.

---

## API contract (Stage 0)

Stage 0 should keep the client/backend integration extremely boring and stable.

### Suggested endpoints

**Fetch a battle**

* `POST /v1/battles:next`
* Request:

  * `client_version`, `session_id` (client-generated UUID), optional `player_id` (anonymous for Stage 0)
* Response:

  * `battle_id`
  * `left`: `{level_id, generator, format, level_payload, ...}`
  * `right`: same structure
  * optional: `instructions`, `allowed_actions` (if needed)

**Submit a vote**

* `POST /v1/votes`
* Body:

  * `battle_id`
  * `result`: `LEFT | RIGHT | TIE | SKIP`
  * optional `left_tags`, `right_tags` (per-level tags)
  * optional `telemetry` object

**Leaderboard**

* `GET /v1/leaderboard`
* Returns sorted generator ratings with match counts (and optional confidence).

### Key invariants (Stage 0)

* A `battle_id` can be voted on **once**.
* Votes must be **idempotent** (client retry shouldn’t double-count).
* Backend must validate that both levels in battle exist and belong to known generators.
* All writes are atomic (vote + rating update in one transaction).

Leaderboard should be displayed on localhost by the backend.

---

## Matchmaking (how battles are chosen)

Stage 0 can be simple but should avoid obvious bias:

**Default strategy**

* Uniformly sample two different generators.
* Uniformly sample one level from each generator’s pool.
* Avoid repeating the exact same pair too often (optional).


---

## Local persistence ✅

**Implemented:** Database persists via bind-mount.

The database file `db/local/arena.sqlite` survives:
- Container restarts (`docker compose down && docker compose up`)
- Container rebuilds (`docker compose up --build`)
- Host system reboots

**How it works:**
- Host directory `./db/local` is mounted to `/data` in container
- Backend writes to `/data/arena.sqlite`
- File remains on host filesystem even when container is removed

**Verify persistence:**
1. Start backend, check leaderboard
2. Stop container: `docker compose down`
3. Restart: `docker compose up`
4. Leaderboard shows same data (generators still at rating 1000.0)

---

## Quickstart (local)

### Prerequisites

* Docker + Docker Compose
* (Optional) Python 3.12+ for local development without Docker
* (Optional) Java 11+ for Java client validation

### Run backend + database

```bash
# From repository root
docker compose up --build
```

Backend starts on `http://localhost:8080`

### Test the implemented features

**View leaderboard:**
```bash
# Open in browser
http://localhost:8080/

# Or fetch JSON
curl http://localhost:8080/v1/leaderboard
```

**Check health:**
```bash
curl http://localhost:8080/health
```

**Run demo script (10 battles + votes):**
```bash
# Bash (Linux/Mac/Docker)
./backend/scripts/demo.sh

# PowerShell (Windows)
.\backend\scripts\demo.ps1
```

**View logs:**
```bash
docker compose logs backend
```

You should see:
- 3 generators imported
- 30 levels imported
- Database ready message with counts

### Persistence test

```bash
# Stop backend
docker compose down

# Restart
docker compose up

# Leaderboard data persists (check http://localhost:8080/)
```

### Reset database

```bash
# Stop and remove volumes
docker compose down

# Delete database file
rm db/local/arena.sqlite

# Restart (recreates from scratch)
docker compose up --build
```

---

## Current implementation status

### Stage 0: ✅ COMPLETE

All core components have been implemented and tested.

#### Backend API (Complete)

| Endpoint | Status | Description |
|----------|--------|-------------|
| `GET /health` | ✅ | Health check with protocol version |
| `GET /` | ✅ | HTML leaderboard for browsers |
| `GET /v1/leaderboard` | ✅ | JSON leaderboard API |
| `POST /v1/battles:next` | ✅ | Issue new battle with two levels |
| `POST /v1/votes` | ✅ | Submit vote, update ELO ratings atomically |
| `GET /debug/db-status` | ✅ | Database statistics (debug mode) |
| `GET /debug/battles` | ✅ | List battles (debug mode) |
| `GET /debug/votes` | ✅ | List votes (debug mode) |

#### Infrastructure (Complete)

| Component | Status | Description |
|-----------|--------|-------------|
| **Database** | ✅ | SQLite with 7 tables, migrations, indexes |
| **Persistence** | ✅ | Docker volume mount, survives restarts |
| **Seed data** | ✅ | 3 generators, 30 levels |
| **ELO ratings** | ✅ | Atomic updates, audit trail |
| **Migrations** | ✅ | Auto-apply on startup |

#### Java Client (Validation Prototype - Complete)

| Component | Status | Description |
|-----------|--------|-------------|
| **Phase 1: Static viewer** | ✅ | Battle fetching, tilemap rendering, voting |
| **Phase 2: Gameplay** | ✅ | Full Mario physics, telemetry collection |
| **API integration** | ✅ | Protocol validation, error handling |
| **Game engine** | ✅ | Ported from Mario AI Framework |
| **UI/UX** | ✅ | Sequential play (left then right), vote submission |

**Note:** The Java client was built as a validation prototype to prove the end-to-end loop. Future Stage 1+ will use a browser-based frontend (no download required).

### Demo validation

Run the demo script to validate the complete flow:

```powershell
# Windows
.\backend\scripts\demo.ps1

# Runs 10 battles with random votes
# Verifies: battle creation, vote submission, rating updates, persistence
```

### Running the Java client (optional validation)

The Java client validates the full gameplay loop locally:

```bash
# Terminal 1: Start backend
docker compose up --build

# Terminal 2: Run Java client
cd client-java
./gradlew run
# Or on Windows: gradlew.bat run
```

**Gameplay flow:**
1. Click "Next Battle" to fetch two levels
2. Press SPACE in left panel to play left level
3. Press SPACE in right panel to play right level
4. Vote (Left Better / Right Better / Tie / Skip)
5. Repeat

**Controls:**
- Arrow keys: Move
- S: Jump
- A: Run/Fire

See `client-java/README.md` and `client-java/GAMEPLAY.md` for details.

---

## Stage 0 completion status

**Status: ✅ COMPLETE**

All Stage 0 acceptance criteria have been met:

- ✅ **10+ battles end-to-end**: Demo script validates full loop
- ✅ **Ratings update correctly**: ELO calculation implemented and tested
- ✅ **Persistence works**: SQLite survives container restarts
- ✅ **Audit trail**: All votes and rating changes logged
- ✅ **Gameplay validated**: Java client proves level playability

Adding a new generator requires:
1. Add entry to `db/seed/generators.json`
2. Create directory `db/seed/levels/{generator_id}/`
3. Add level files (ASCII tilemap format, 16 lines × variable width)
4. Restart backend: `docker compose up --build`

---

## Relationship to the broader Mario PCG ecosystem

This repo sits at the intersection of:

* **PCG research** (generators, representations, controllability, quality, diversity),
* **human evaluation** (preferences, fun, frustration),
* and the tradition of Mario evaluation frameworks originating from the **Mario AI competitions** and related tooling (notably Java-based environments). 

Long-term, this can complement purely automated metrics (e.g., playability via A* agents) by providing the thing that is often missing: **human preference data at scale**.

## Roadmap (next stages)

### Stage 1 (MVP hosted) — Planning complete ✅

Deploy backend to cloud hosting for remote testing:

* **Target:** Small tester group (~10-50 users)
* **Backend:** Single VM with Docker (GCP e2-micro free tier recommended)
* **Database:** SQLite on VM persistent disk
* **Frontend:** Browser-based client (no download required)
* **Backups:** Automated daily backups
* **Cost:** $0-10/month (free tier or cheap VPS)

See `docs/stage1-spec.md` for:
- Cloud platform cost comparison (GCP, AWS, Azure, DigitalOcean, Fly.io, Hetzner)
- Complete deployment guide (GCP step-by-step)
- Implementation tasks (backend changes, web portal, admin tools)
- Backup and monitoring setup

### Stage 2 (Research utility)

* Advanced matchmaking (skill-based, diversity-promoting)
* Generator versioning and A/B testing
* Exportable preference dataset for publications
* Data visualization and analysis tools

### Stage 3 (Community platform)

* User accounts and authentication
* Browser-playable Mario (JavaScript/WebGL)
* Moderation and anti-abuse systems
* Sandboxed generator submissions
* API for external researchers

Stage 0 is intentionally narrow: it exists to prove the loop, not to build a community platform immediately.

---

## License and contribution philosophy

* Stage 0 is built to be open-source friendly: clear API, clear data model, minimal dependencies.
* Contributions later should focus on:

  * new generators (as level bundles in Stage 0),
  * new rating methods (without breaking stored data),
  * UI/UX improvements in the Java client.
