# PCG Arena (Stage 0) ‚Äî Local Pairwise Rating for Mario PCG Generators

## What this repository is

This repository implements **PCG Arena ‚Äî Stage 0**: a **local-only**, end-to-end prototype of a platform that lets a human player **play two Mario levels**, choose which one is better, and updates a **leaderboard of generators** based on those pairwise preferences.

Here we build the **arena infrastructure** that can evaluate *any* levels created via Procedural Content Generator by collecting human preference data.

### Why Stage 0 matters

Before building a web service or trying to attract users, Stage 0 validates the core loop:

1. **Pick two levels** (each attributed to a generator).
2. Player plays both inside a **Java client**.
3. Player submits a **preference vote** (Left wins / Right wins / Tie / Skip).
4. Backend updates ratings and persists everything so restarting does **not** lose history.

This is directly inspired by the general idea used in the **Mario AI Championship ‚ÄúLevel Generation Track‚Äù**, where humans play content generated by different generators and choose what they prefer. 

---

## Project goals (Stage 0)

### Primary goal

**Prove the core preference-based evaluation loop works locally and reproducibly.**

### Secondary goals

* Make the system **extensible**: adding a new generator levels should be ‚Äúdrop in new level set + metadata‚Äù.
* Keep the design close to what a hosted MVP would need later (clean API boundaries, persistent storage, audit logs).
* Collect clean data suitable for later research (pairwise comparisons + metadata + session events).

---

## Stage 0 scope (what‚Äôs in / what‚Äôs out)

### In scope

* Local backend in a **Docker container**: API + rating logic.
* Local database persisted **outside** the container (Docker volume or bind mount).
* Local Java client executable that:

  * fetches a ‚Äúbattle‚Äù (two candidate levels),
  * renders and runs gameplay,
  * collects a vote + optional tags,
  * submits results.

### Out of scope (explicitly not Stage 0)

* Public hosting, accounts, OAuth, anti-cheat at scale.
* Browser-playable Mario.
* Uploading arbitrary generator code from users (sandboxing).
* ‚ÄúContinuous generation‚Äù (Stage 0 assumes a fixed pool of levels on disk).

---

## Architecture (Stage 0)

Stage 0 is three components with **hard boundaries** so each can be developed independently:

1. **Client (Java game executable)**
2. **Backend (Docker container)**
3. **Database (persistent local storage outside the container)**

### 1) Java Client responsibilities

* Treat the backend as a black box.
* Ask for a battle: ‚ÄúGive me two levels to compare.‚Äù
* Run both levels with identical UX structure (so comparisons are fair).
* Collect:

  * **vote**: LEFT/RIGHT/TIE/SKIP
  * optional: completion, deaths, time, subjective tags
* Submit vote to backend and show updated rating snapshot (or at least confirmation).

### 2) Backend responsibilities

* Own all business logic:

  * selecting battles,
  * tracking sessions,
  * validating vote payloads,
  * updating ratings,
  * writing everything to the DB.
* Serve a stable API contract so the Java client doesn‚Äôt need to know internals.
* Provide admin/debug endpoints locally (optional but useful).

### 3) Database responsibilities

* Persist:

  * generators and their metadata,
  * levels and attribution,
  * battles served,
  * votes received,
  * rating history (or at least current rating + match count).
* Must survive:

  * backend container restart,
  * backend image rebuild.

---

## Repository layout

### Current structure (Stage 0)

```
pcg-arena/
  README.md
  docker-compose.yml       # ‚úÖ One-command setup (docker compose up)
  .gitignore               # ‚úÖ Excludes db/local/, client-java/dist/

  backend/                 # ‚úÖ Dockerized Python/FastAPI backend
    spec.md                # ‚úÖ Backend implementation documentation
    Dockerfile             # ‚úÖ Container build instructions
    requirements.txt       # ‚úÖ Python dependencies (FastAPI, Uvicorn)
    config/
      settings.env         # ‚úÖ Environment variable defaults
    src/
      __init__.py          # ‚úÖ Package metadata
      config.py            # ‚úÖ Configuration loader
      main.py              # ‚úÖ FastAPI app + endpoints
      db/
        __init__.py        # ‚úÖ Module exports
        connection.py      # ‚úÖ SQLite connection management
        migrations.py      # ‚úÖ Migration runner
        seed.py            # ‚úÖ Seed importer + validation
    tests/                 # ‚è≥ Placeholder
    openapi/               # ‚è≥ Placeholder
    scripts/               # ‚è≥ Placeholder

  db/
    spec.md                # ‚úÖ Database specification
    migrations/            # ‚úÖ SQL migration scripts
      001_init.sql         # ‚úÖ Core tables (7 tables)
      002_indexes.sql      # ‚úÖ Performance indexes
    seed/                  # ‚úÖ Initial dataset
      generators.json      # ‚úÖ 3 generators (hopper, genetic, notch)
      levels/              # ‚úÖ 30 level files (10 per generator)
        genetic/
        hopper/
        notch/
    local/                 # ‚úÖ Runtime storage (gitignored)
      arena.sqlite         # ‚úÖ SQLite database (created on first run)

  client-java/             # ‚úÖ Java client (validation prototype)
    spec.md                # ‚úÖ Client specification
    build.gradle           # ‚úÖ Gradle build configuration
    src/
      main/java/arena/
        api/               # ‚úÖ HTTP client + models
        config/            # ‚úÖ Configuration loader
        game/              # ‚úÖ Mario game engine (ported from Framework)
        ui/                # ‚úÖ Swing UI (gameplay panels, voting)
        util/              # ‚úÖ JSON, logging, time utilities
      main/resources/
        img/               # ‚úÖ Mario sprite sheets
        logback.xml        # ‚úÖ Logging configuration
    logs/                  # Runtime logs (gitignored)
    build/                 # Build artifacts (gitignored)

  frontend/                # ‚úÖ Browser client (React + TypeScript)
    spec.md                # ‚úÖ Frontend specification
    package.json           # ‚úÖ npm dependencies
    vite.config.ts         # ‚úÖ Vite build configuration
    src/
      api/                 # ‚úÖ API client + types
      engine/              # ‚úÖ Mario game engine (TypeScript port)
        sprites/           # ‚úÖ Mario, enemies, items
        graphics/          # ‚úÖ Rendering system (Canvas)
        effects/           # ‚úÖ Visual effects
        input/             # ‚úÖ Keyboard controls
      components/          # ‚úÖ React components (BattleFlow, GameCanvas, etc.)
      styles/              # ‚úÖ CSS stylesheets
    public/
      assets/              # ‚úÖ Sprite sheets (png files)
    dist/                  # Production build (gitignored)

  shared/                  # ‚è≥ Placeholder
    schemas/
    level-format/

  docs/
    stage0-spec.md         # ‚úÖ Detailed technical specification
    stage1-spec.md         # ‚úÖ Stage 1 deployment specification
    stage2-spec.md         # ‚úÖ Stage 2 browser frontend specification
    PROJECT-STATUS.md      # ‚úÖ Current project status
    future-notes.md        # ‚úÖ Future roadmap notes
  
  Mario-AI-Framework-PCG/  # ‚úÖ Source of game engine and levels
    src/engine/            # ‚úÖ Mario physics (ported to client-java)
    levels/                # ‚úÖ 9000+ generated levels (not imported yet)
    img/                   # ‚úÖ Sprite sheets (copied to client-java)
```

**Legend:**  
‚úÖ Implemented | ‚è≥ Placeholder / Not yet implemented

### Notes

* `docker-compose.yml` orchestrates backend + volume mounts
* `db/local/` and `client-java/dist/` are gitignored
* Backend serves HTTP API on `localhost:8080`
* Database persists across container restarts

---

## Stage 0 data model (conceptual)

Minimum entities:

* **Generator**

  * `generator_id`, `name`, `version`, `description`, `tags`, `documentation-url`
* **Level**

  * `level_id`, `generator_id`, `content_blob`, `content_format`, `hash`
* **Battle**

  * `battle_id`, `left_level_id`, `right_level_id`, `served_at`, `session_id`
* **Vote**

  * `vote_id`, `battle_id`, `result` (LEFT/RIGHT/TIE/SKIP), `created_at`
  * optional telemetry: `left_tags`, `right_tags`, `telemetry_json`
* **Rating**

  * `generator_id`, `rating_value`, `rating_deviation` (optional), `games_played`

Stage 0 should also store **immutable logs** (battle served ‚Üí vote received) so you can debug issues and later publish datasets.

---

## Rating system (Stage 0 recommendation)

Because the platform is about **pairwise human preference**, you want a rating that:

* updates online,
* supports uncertainty / cold-start,
* is easy to explain.

### Minimal implementation: Elo (pairwise winners)

* Treat each generator as a ‚Äúplayer‚Äù.
* Each vote is a ‚Äúmatch‚Äù between two generators.
* Outcome: win/loss/tie/skip.
* Update Elo after each match.

This gives you the ‚ÄúELO-like generator leaderboard‚Äù you want with extremely low complexity.

---

## Level format (Stage 0)

Stage 0 only needs a format that:

* the Java client can load quickly,
* the backend can treat as opaque bytes + metadata,
* supports a fixed set of tile types.

We will use **ASCII grid / tilemap** (easy, transparent)

It will always be a rectangle of variable width (up to 250) by 16 (LEVEL_HEIGHT) lines, consisting of ASCII characters. E.g. `-` denotes air, `X` ground, `t` pipe or `o` coin.

---

## API contract (Stage 0)

Stage 0 should keep the client/backend integration extremely boring and stable.

### Suggested endpoints

**Fetch a battle**

* `POST /v1/battles:next`
* Request:

  * `client_version`, `session_id` (client-generated UUID), optional `player_id` (anonymous for Stage 0)
* Response:

  * `battle_id`
  * `left`: `{level_id, generator, format, level_payload, ...}`
  * `right`: same structure
  * optional: `instructions`, `allowed_actions` (if needed)

**Submit a vote**

* `POST /v1/votes`
* Body:

  * `battle_id`
  * `result`: `LEFT | RIGHT | TIE | SKIP`
  * optional `left_tags`, `right_tags` (per-level tags)
  * optional `telemetry` object

**Leaderboard**

* `GET /v1/leaderboard`
* Returns sorted generator ratings with match counts (and optional confidence).

### Key invariants (Stage 0)

* A `battle_id` can be voted on **once**.
* Votes must be **idempotent** (client retry shouldn‚Äôt double-count).
* Backend must validate that both levels in battle exist and belong to known generators.
* All writes are atomic (vote + rating update in one transaction).

Leaderboard should be displayed on localhost by the backend.

---

## Matchmaking (how battles are chosen)

Stage 0 can be simple but should avoid obvious bias:

**Default strategy**

* Uniformly sample two different generators.
* Uniformly sample one level from each generator‚Äôs pool.
* Avoid repeating the exact same pair too often (optional).


---

## Local persistence ‚úÖ

**Implemented:** Database persists via bind-mount.

The database file `db/local/arena.sqlite` survives:
- Container restarts (`docker compose down && docker compose up`)
- Container rebuilds (`docker compose up --build`)
- Host system reboots

**How it works:**
- Host directory `./db/local` is mounted to `/data` in container
- Backend writes to `/data/arena.sqlite`
- File remains on host filesystem even when container is removed

**Verify persistence:**
1. Start backend, check leaderboard
2. Stop container: `docker compose down`
3. Restart: `docker compose up`
4. Leaderboard shows same data (generators still at rating 1000.0)

---

## Quickstart (local)

### Prerequisites

* Docker + Docker Compose
* (Optional) Python 3.12+ for local development without Docker
* (Optional) Java 11+ for Java client validation

### Run backend + database

```bash
# From repository root
docker compose up --build
```

Backend starts on `http://localhost:8080`

### Test the implemented features

**View leaderboard:**
```bash
# Open in browser
http://localhost:8080/

# Or fetch JSON
curl http://localhost:8080/v1/leaderboard
```

**Check health:**
```bash
curl http://localhost:8080/health
```

**Run demo script (10 battles + votes):**
```bash
# Bash (Linux/Mac/Docker)
./backend/scripts/demo.sh

# PowerShell (Windows)
.\backend\scripts\demo.ps1
```

**View logs:**
```bash
docker compose logs backend
```

You should see:
- 3 generators imported
- 30 levels imported
- Database ready message with counts

### Persistence test

```bash
# Stop backend
docker compose down

# Restart
docker compose up

# Leaderboard data persists (check http://localhost:8080/)
```

### Reset database

```bash
# Stop and remove volumes
docker compose down

# Delete database file
rm db/local/arena.sqlite

# Restart (recreates from scratch)
docker compose up --build
```

---

## Current implementation status

### Stage 0: ‚úÖ COMPLETE

All core components have been implemented and tested.

#### Backend API (Complete)

| Endpoint | Status | Description |
|----------|--------|-------------|
| `GET /health` | ‚úÖ | Health check with protocol version |
| `GET /` | ‚úÖ | HTML leaderboard for browsers |
| `GET /v1/leaderboard` | ‚úÖ | JSON leaderboard API |
| `POST /v1/battles:next` | ‚úÖ | Issue new battle with two levels |
| `POST /v1/votes` | ‚úÖ | Submit vote, update ELO ratings atomically |
| `GET /debug/db-status` | ‚úÖ | Database statistics (debug mode) |
| `GET /debug/battles` | ‚úÖ | List battles (debug mode) |
| `GET /debug/votes` | ‚úÖ | List votes (debug mode) |

#### Infrastructure (Complete)

| Component | Status | Description |
|-----------|--------|-------------|
| **Database** | ‚úÖ | SQLite with 7 tables, migrations, indexes |
| **Persistence** | ‚úÖ | Docker volume mount, survives restarts |
| **Seed data** | ‚úÖ | 3 generators, 30 levels |
| **ELO ratings** | ‚úÖ | Atomic updates, audit trail |
| **Migrations** | ‚úÖ | Auto-apply on startup |

#### Java Client (Validation Prototype - Complete)

| Component | Status | Description |
|-----------|--------|-------------|
| **Phase 1: Static viewer** | ‚úÖ | Battle fetching, tilemap rendering, voting |
| **Phase 2: Gameplay** | ‚úÖ | Full Mario physics, telemetry collection |
| **API integration** | ‚úÖ | Protocol validation, error handling |
| **Game engine** | ‚úÖ | Ported from Mario AI Framework |
| **UI/UX** | ‚úÖ | Sequential play (left then right), vote submission |

**Note:** The Java client was built as a validation prototype to prove the end-to-end loop. Future Stage 1+ will use a browser-based frontend (no download required).

### Demo validation

Run the demo script to validate the complete flow:

```powershell
# Windows
.\backend\scripts\demo.ps1

# Runs 10 battles with random votes
# Verifies: battle creation, vote submission, rating updates, persistence
```

### Running the Java client (optional validation)

The Java client validates the full gameplay loop locally:

```bash
# Terminal 1: Start backend
docker compose up --build

# Terminal 2: Run Java client
cd client-java
./gradlew run
# Or on Windows: gradlew.bat run
```

**Gameplay flow:**
1. Click "Next Battle" to fetch two levels
2. Press SPACE in left panel to play left level
3. Press SPACE in right panel to play right level
4. Vote (Left Better / Right Better / Tie / Skip)
5. Repeat

**Controls:**
- Arrow keys: Move
- S: Jump
- A: Run/Fire

See `client-java/README.md` and `client-java/GAMEPLAY.md` for details.

---

## Stage 0 completion status

**Status: ‚úÖ COMPLETE**

All Stage 0 acceptance criteria have been met:

- ‚úÖ **10+ battles end-to-end**: Demo script validates full loop
- ‚úÖ **Ratings update correctly**: ELO calculation implemented and tested
- ‚úÖ **Persistence works**: SQLite survives container restarts
- ‚úÖ **Audit trail**: All votes and rating changes logged
- ‚úÖ **Gameplay validated**: Java client proves level playability

Adding a new generator requires:
1. Add entry to `db/seed/generators.json`
2. Create directory `db/seed/levels/{generator_id}/`
3. Add level files (ASCII tilemap format, 16 lines √ó variable width)
4. Restart backend: `docker compose up --build`

---

## Relationship to the broader Mario PCG ecosystem

This repo sits at the intersection of:

* **PCG research** (generators, representations, controllability, quality, diversity),
* **human evaluation** (preferences, fun, frustration),
* and the tradition of Mario evaluation frameworks originating from the **Mario AI competitions** and related tooling (notably Java-based environments). 

Long-term, this can complement purely automated metrics (e.g., playability via A* agents) by providing the thing that is often missing: **human preference data at scale**.

## Roadmap

### Stage 0 (Concept validation) ‚Äî ‚úÖ COMPLETE

**Status:** Complete and validated locally

* Local-only prototype proving end-to-end loop
* Docker backend with SQLite persistence
* ELO rating system with atomic updates
* Java client with full Mario gameplay
* 30 seed levels (10 per generator: genetic, hopper, notch)

**Deliverables achieved:**
- ‚úÖ Battle assignment ‚Üí play levels ‚Üí vote ‚Üí update leaderboard
- ‚úÖ Data persists across restarts
- ‚úÖ Complete API implementation (8 endpoints)
- ‚úÖ Java client validation (Phases 1 & 2)
- ‚úÖ Demo scripts for automated testing

---

### Stage 1 (Cloud deployment) ‚Äî ‚úÖ COMPLETE

**Status:** Deployed and operational

Deploy backend to cloud hosting for remote validation:

* **Deployed to:** GCP Compute Engine (e2-micro free tier)
* **Backend:** Single VM with Docker
* **Database:** SQLite on VM persistent disk
* **Backups:** Automated daily backups configured
* **Cost:** ~$3-4/month (static IP only)

**Deliverables achieved:**
- ‚úÖ CORS headers for browser access
- ‚úÖ Environment-based configuration (11 env variables)
- ‚úÖ Enhanced health check with metrics
- ‚úÖ Request logging middleware
- ‚úÖ Rate limiting (10/min battles, 20/min votes)
- ‚úÖ Admin endpoints with Bearer token auth
- ‚úÖ Backup/restore scripts (Windows & Linux)
- ‚úÖ Remote connectivity validated with Java client

See `docs/stage1-spec.md` for:
- Cloud platform cost comparison
- Complete GCP deployment guide
- Operational procedures and monitoring

---

### Stage 2 (Browser frontend) ‚Äî ‚úÖ COMPLETE

**Status:** Complete and operational (2025-12-26)

**Deliverables achieved:**
* Browser-playable Mario (TypeScript/React + Canvas)
* Embedded gameplay with identical UX to Java client
* Same protocol (`arena/v0`) - no backend changes needed
* Instant "battle" flow without installation
* Desktop keyboard controls (Arrow keys, S, A)

**Technical implementation:**
- ‚úÖ Mario engine ported to TypeScript (faithful recreation)
- ‚úÖ React components for battle flow and UI
- ‚úÖ HTML5 Canvas rendering system
- ‚úÖ Full telemetry collection matching Java client
- ‚úÖ Side-by-side level comparison layout
- ‚úÖ Generator name reveal only after voting
- ‚úÖ Production build tested and ready

**Key Features:**
- No download required - runs in any modern browser
- 30 FPS gameplay with faithful physics
- Desktop-focused (keyboard controls)
- ~150 KB gzipped bundle size
- Cross-browser support (Chrome, Firefox, Edge, Safari)

See `docs/stage2-spec.md` and `frontend/spec.md` for complete documentation.

---

### Stage 3 (Builder Profile) ‚Äî ‚úÖ COMPLETE & DEPLOYED

**Status:** Complete and deployed to production (2025-12-28)

**Purpose:** Enable researchers to submit their own generators

**Deliverables achieved:**
* **Authentication System:**
  - Google OAuth integration (production-ready)
  - Email/password registration and login
  - Email verification with SendGrid
  - Password reset flow
  - Secure session management with HTTP-only cookies
  - Dev auth for local testing
* **Builder Profile:**
  - Generator management dashboard
  - Generator submission via ZIP upload (50-200 levels required)
  - Generator updates with version tracking (preserves rating and existing battles)
  - Soft delete for generators with battles (maintains data integrity)
  - Up to 3 generators per user
  - Immediate leaderboard integration
* **Generator View:**
  - Dedicated page for each generator with detailed information
  - Visual level gallery with static previews of all levels
  - Level previews show tiles and enemies using original Mario graphics
  - Accessible from Leaderboard (clickable generator names) and Builder Profile
* **Enhanced Battle Experience:**
  - Level previews on voting page (both levels displayed before voting)
  - A/B naming (Level A, Level B) instead of directional names
  - Generator names revealed inline after voting with auto-advance
  - Dedicated Leaderboard page separate from battle flow
* **User Experience:**
  - Email verification required for submissions
  - OAuth users auto-verified
  - Forgot password functionality
  - Persistent login with cookies

**Technical implementation:**
- Database: `users`, `user_sessions`, `email_verification_tokens`, `password_reset_tokens` tables
- Backend: Complete `/v1/auth/*` and `/v1/builders/*` API, `/v1/generators/{id}` endpoint
- Frontend: React Router, AuthContext, BuilderPage with Google Sign-In, GeneratorPage, LevelPreview component
- Email: SendGrid integration for verification and password reset
- Security: Bcrypt password hashing, token-based verification
- UI: Level preview system using Canvas rendering with original Mario graphics

See `docs/stage3-spec.md` for complete documentation.

---

### Stage 4 (Platform refinement) ‚Äî ‚úÖ COMPLETE (2025-12-30)

**Purpose:** Turn platform into research-grade utility

**Status:** Phase 1 complete and deployed

#### Stage 4a: AGIS Matchmaking (‚úÖ Complete)
- **Glicko-2 Rating System:** Replaced Elo with Glicko-2 for uncertainty-aware ratings
  - Rating deviation (RD) and volatility tracking
  - Better handling of new generators with limited data
- **AGIS Algorithm:** Adaptive Glicko-Informed Selection for intelligent matchmaking
  - Fast convergence: Prioritizes generators with high uncertainty
  - Similar ratings: Prefers matchups between generators of similar skill
  - Coverage: Ensures all generator pairs get sufficient battles
  - Quality bias: Slightly favors better generators after convergence
- **Generator Pair Statistics:** Tracks battle history between specific pairs
- **Configurable Parameters:** All AGIS parameters configurable via environment variables

#### Stage 4b: Coverage System (‚úÖ Complete)
- **Confusion Matrix Endpoint:** `GET /v1/stats/confusion-matrix` for pairwise comparison data
  - Win rates and battle counts between all generator pairs
  - Coverage statistics (pairs with data, pairs at target)
  - Publicly accessible for research analysis
- **Admin Dashboard:** OAuth-protected admin interface
  - Matchmaking statistics and system configuration
  - Generator status and convergence tracking
  - Coverage gaps analysis (under-covered and missing pairs)
  - Three-tab interface: Overview, Confusion Matrix, Coverage Gaps
  - Accessible only to authorized admin emails via Google OAuth
- **Configurable AGIS Parameters:**
  - `ARENA_AGIS_MIN_GAMES`: Games before generator is "converged" (default: 30)
  - `ARENA_AGIS_TARGET_BATTLES_PER_PAIR`: Minimum battles per pair (default: 10)
  - `ARENA_AGIS_RATING_SIGMA`: Rating similarity standard deviation (default: 150.0)
  - `ARENA_AGIS_QUALITY_BIAS`: How much to favor high-rated generators (default: 0.2)

**Deliverables achieved:**
- ‚úÖ Uncertainty-aware matchmaking (AGIS + Glicko-2)
- ‚úÖ Coverage tracking and visualization
- ‚úÖ Admin dashboard for system monitoring
- ‚úÖ Configurable matchmaking behavior
- ‚úÖ Research-ready data export (confusion matrix)

---

### Stage 5 (Research Analytics) ‚Äî üìã PLANNED

**Purpose:** Enable research-grade data collection for academic publication

**Status:** Specification complete, implementation pending

#### Stage 5a: Enhanced Data Collection
- **Enhanced Telemetry:** Full event streams, position trajectories, death locations
- **Anonymous Player IDs:** Persistent tracking via localStorage + cookie fallback
- **Per-Level Statistics:** Win rates, completion rates, tag distributions per level
- **Static Level Features:** Structural analysis (gaps, enemies, platforms, complexity)

#### Stage 5b: Public Statistics
- **Platform Statistics Page:** Total battles, vote distribution, engagement metrics
- **Enhanced Generator Page:** Performance charts, tag distributions, consistency scores
- **Level Detail Page:** Deep-dive with death heatmaps, structural analysis
- **Level Gallery Sorting:** Sort by win rate, difficulty, times played

#### Stage 5c: Admin Data Export
- **Research Dataset Export:** CSV/JSON download of all data
- **Trajectory Export:** Position data for heatmap analysis
- **Player Profile Export:** Anonymous preference patterns
- **Date Range Filtering:** Export specific time periods

**Research Hypotheses Enabled:**
- H1: Moderate difficulty maximizes engagement
- H2: Path variability correlates with higher ratings
- H3: Player preferences cluster into distinct types
- H4: Tags can be predicted from gameplay telemetry
- H5: Generators have distinguishable gameplay signatures

See `docs/stage5-spec.md` for complete specification.

---

## License and contribution philosophy

* Stage 0 is built to be open-source friendly: clear API, clear data model, minimal dependencies.
* Contributions later should focus on:

  * new generators (as level bundles in Stage 0),
  * new rating methods (without breaking stored data),
  * UI/UX improvements in the Java client.
