# PCG Arena (Stage 0) — Local Pairwise Rating for Mario PCG Generators

## What this repository is

This repository implements **PCG Arena — Stage 0**: a **local-only**, end-to-end prototype of a platform that lets a human player **play two Mario levels**, choose which one is better, and updates a **leaderboard of generators** based on those pairwise preferences.

Here we build the **arena infrastructure** that can evaluate *any* levels created via Procedural Content Generator by collecting human preference data.

### Why Stage 0 matters

Before building a web service or trying to attract users, Stage 0 validates the core loop:

1. **Pick two levels** (each attributed to a generator).
2. Player plays both inside a **Java client**.
3. Player submits a **preference vote** (Left wins / Right wins / Tie / Skip).
4. Backend updates ratings and persists everything so restarting does **not** lose history.

This is directly inspired by the general idea used in the **Mario AI Championship “Level Generation Track”**, where humans play content generated by different generators and choose what they prefer. 

---

## Project goals (Stage 0)

### Primary goal

**Prove the core preference-based evaluation loop works locally and reproducibly.**

### Secondary goals

* Make the system **extensible**: adding a new generator levels should be “drop in new level set + metadata”.
* Keep the design close to what a hosted MVP would need later (clean API boundaries, persistent storage, audit logs).
* Collect clean data suitable for later research (pairwise comparisons + metadata + session events).

---

## Stage 0 scope (what’s in / what’s out)

### In scope

* Local backend in a **Docker container**: API + rating logic.
* Local database persisted **outside** the container (Docker volume or bind mount).
* Local Java client executable that:

  * fetches a “battle” (two candidate levels),
  * renders and runs gameplay,
  * collects a vote + optional tags,
  * submits results.

### Out of scope (explicitly not Stage 0)

* Public hosting, accounts, OAuth, anti-cheat at scale.
* Browser-playable Mario.
* Uploading arbitrary generator code from users (sandboxing).
* “Continuous generation” (Stage 0 assumes a fixed pool of levels on disk).

---

## Architecture (Stage 0)

Stage 0 is three components with **hard boundaries** so each can be developed independently:

1. **Client (Java game executable)**
2. **Backend (Docker container)**
3. **Database (persistent local storage outside the container)**

### 1) Java Client responsibilities

* Treat the backend as a black box.
* Ask for a battle: “Give me two levels to compare.”
* Run both levels with identical UX structure (so comparisons are fair).
* Collect:

  * **vote**: left/right/tie/skip
  * optional: completion, deaths, time, subjective tags
* Submit vote to backend and show updated rating snapshot (or at least confirmation).

### 2) Backend responsibilities

* Own all business logic:

  * selecting battles,
  * tracking sessions,
  * validating vote payloads,
  * updating ratings,
  * writing everything to the DB.
* Serve a stable API contract so the Java client doesn’t need to know internals.
* Provide admin/debug endpoints locally (optional but useful).

### 3) Database responsibilities

* Persist:

  * generators and their metadata,
  * levels and attribution,
  * battles served,
  * votes received,
  * rating history (or at least current rating + match count).
* Must survive:

  * backend container restart,
  * backend image rebuild.

---

## Repository layout

A simple structure that keeps boundaries clean:

```
pcg-arena/
  README.md

  backend/                 # Dockerized API + rating logic
    Dockerfile
    src/
    tests/
    config/
    openapi/               # OpenAPI spec (recommended)
    scripts/               # local dev scripts (seed db, etc.)

  db/
    migrations/            # schema migrations
    seed/                  # initial dataset: generators + levels
    local/                 # (gitignored) local db files if bind-mounted

  client-java/
    gradle/ or maven/
    src/
    assets/
      levels/              # optional: cached levels for offline dev
    dist/                  # built runnable artifacts (gitignored)

  shared/
    schemas/               # shared JSON schemas for payload validation
    level-format/          # definition + examples for level encoding

  docs/
    stage0-spec.md         # detailed spec (API, data model, UX flow)
    rating.md              # rating algorithm rationale
    threat-model-lite.md   # local abuse assumptions + future notes
    decisions.md           # ADR-style decisions (short)
```

Notes:

* If you prefer **one-command local run**, add a `docker-compose.yml` at repo root to run backend + DB volume.
* Keep `db/local/` and `client-java/dist/` **gitignored**.

---

## Stage 0 data model (conceptual)

Minimum entities:

* **Generator**

  * `generator_id`, `name`, `version`, `description`, `tags`, `documentation-url`
* **Level**

  * `level_id`, `generator_id`, `content_blob`, `content_format`, `hash`
* **Battle**

  * `battle_id`, `left_level_id`, `right_level_id`, `served_at`, `client_session_id`
* **Vote**

  * `vote_id`, `battle_id`, `result` (L/R/Tie/Skip), `created_at`
  * optional telemetry: `time_left`, `time_right`, `deaths_left`, `deaths_right`, `completed_left/right`
* **Rating**

  * `generator_id`, `rating_value`, `rating_deviation` (optional), `games_played`

Stage 0 should also store **immutable logs** (battle served → vote received) so you can debug issues and later publish datasets.

---

## Rating system (Stage 0 recommendation)

Because the platform is about **pairwise human preference**, you want a rating that:

* updates online,
* supports uncertainty / cold-start,
* is easy to explain.

### Minimal implementation: Elo (pairwise winners)

* Treat each generator as a “player”.
* Each vote is a “match” between two generators.
* Outcome: win/loss/tie/skip.
* Update Elo after each match.

This gives you the “ELO-like generator leaderboard” you want with extremely low complexity.

---

## Level format (Stage 0)

Stage 0 only needs a format that:

* the Java client can load quickly,
* the backend can treat as opaque bytes + metadata,
* supports a fixed set of tile types.

We will use **ASCII grid / tilemap** (easy, transparent)

It will always be a rectangel of size 150 (LEVEL_WIDTH) by 16 (LEVEL_HEIGHT) consisting of ASCII characters. E.g. `-` denotes air, `X` ground, `t` pipe or `o` coin.

---

## API contract (Stage 0)

Stage 0 should keep the client/backend integration extremely boring and stable.

### Suggested endpoints

**Fetch a battle**

* `POST /v1/battles:next`
* Request:

  * `client_version`, `session_id` (client-generated UUID), optional `player_id` (anonymous for Stage 0)
* Response:

  * `battle_id`
  * `left`: `{level_id, generator_id, generator_name, level_payload, format}`
  * `right`: same
  * optional: `instructions`, `allowed_actions` (if needed)

**Submit a vote**

* `POST /v1/votes`
* Body:

  * `battle_id`
  * `result`: `LEFT | RIGHT | TIE | SKIP`
  * optional telemetry fields
  * optional tags: `fun`, `frustrating`, `interesting`, `unfair`, etc.

**Leaderboard**

* `GET /v1/leaderboard`
* Returns sorted generator ratings with match counts (and optional confidence).

### Key invariants (Stage 0)

* A `battle_id` can be voted on **once**.
* Votes must be **idempotent** (client retry shouldn’t double-count).
* Backend must validate that both levels in battle exist and belong to known generators.
* All writes are atomic (vote + rating update in one transaction).

Leaderboard should be displayed on localhost by the backend.

---

## Matchmaking (how battles are chosen)

Stage 0 can be simple but should avoid obvious bias:

**Default strategy**

* Uniformly sample two different generators.
* Uniformly sample one level from each generator’s pool.
* Avoid repeating the exact same pair too often (optional).


---

## Local persistence (non-negotiable requirement)

The database must outlive the container, which means:

* Use a Docker volume (recommended), or
* Bind-mount a local directory into the container.

Stage 0 success criteria explicitly depends on stable persistence across restarts.

---

## Quickstart (local)

> The exact commands depend on backend tech choice. This is a “shape” that the repo should converge to.

### Prereqs

* Docker + Docker Compose
* Java 17+ (or whatever your client framework requires)

### Run backend + DB locally

```bash
docker compose up --build
```

### Run the Java client

```bash
cd client-java
./gradlew run
```

### Sanity test flow

1. Client requests a battle
2. You play left level, then right level
3. You vote
4. You fetch leaderboard and see rating changes
5. You restart backend container and confirm the state persists

---

## Stage 0 definition of done (what “finished” means)

A Stage 0 release is “done” when:

* You can run **10 battles end-to-end** without manual DB fiddling.
* Ratings update after every vote and remain consistent after restart.
* Adding a new generator requires **only**:

  * adding generator metadata,
  * dropping a bundle of levels into `db/seed/…`,
  * running a seed/import script.

---

## Relationship to the broader Mario PCG ecosystem

This repo sits at the intersection of:

* **PCG research** (generators, representations, controllability, quality, diversity),
* **human evaluation** (preferences, fun, frustration),
* and the tradition of Mario evaluation frameworks originating from the **Mario AI competitions** and related tooling (notably Java-based environments). 

Long-term, this can complement purely automated metrics (e.g., playability via A* agents) by providing the thing that is often missing: **human preference data at scale**.

## Roadmap (beyond Stage 0)

* **Stage 1 (MVP hosted)**: deploy backend, basic web leaderboard, downloadable client.
* **Stage 2 (research utility)**: better matchmaking, generator versioning, exportable preference dataset.
* **Stage 3 (community platform)**: browser play, accounts, moderation, sandboxed generator submissions.

Stage 0 is intentionally narrow: it exists to prove the loop, not to build a community platform immediately.

---

## License and contribution philosophy

* Stage 0 is built to be open-source friendly: clear API, clear data model, minimal dependencies.
* Contributions later should focus on:

  * new generators (as level bundles in Stage 0),
  * new rating methods (without breaking stored data),
  * UI/UX improvements in the Java client.
