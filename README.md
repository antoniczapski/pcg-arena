# PCG Arena (Stage 0) — Local Pairwise Rating for Mario PCG Generators

## What this repository is

This repository implements **PCG Arena — Stage 0**: a **local-only**, end-to-end prototype of a platform that lets a human player **play two Mario levels**, choose which one is better, and updates a **leaderboard of generators** based on those pairwise preferences.

Here we build the **arena infrastructure** that can evaluate *any* levels created via Procedural Content Generator by collecting human preference data.

### Why Stage 0 matters

Before building a web service or trying to attract users, Stage 0 validates the core loop:

1. **Pick two levels** (each attributed to a generator).
2. Player plays both inside a **Java client**.
3. Player submits a **preference vote** (Left wins / Right wins / Tie / Skip).
4. Backend updates ratings and persists everything so restarting does **not** lose history.

This is directly inspired by the general idea used in the **Mario AI Championship “Level Generation Track”**, where humans play content generated by different generators and choose what they prefer. 

---

## Project goals (Stage 0)

### Primary goal

**Prove the core preference-based evaluation loop works locally and reproducibly.**

### Secondary goals

* Make the system **extensible**: adding a new generator levels should be “drop in new level set + metadata”.
* Keep the design close to what a hosted MVP would need later (clean API boundaries, persistent storage, audit logs).
* Collect clean data suitable for later research (pairwise comparisons + metadata + session events).

---

## Stage 0 scope (what’s in / what’s out)

### In scope

* Local backend in a **Docker container**: API + rating logic.
* Local database persisted **outside** the container (Docker volume or bind mount).
* Local Java client executable that:

  * fetches a “battle” (two candidate levels),
  * renders and runs gameplay,
  * collects a vote + optional tags,
  * submits results.

### Out of scope (explicitly not Stage 0)

* Public hosting, accounts, OAuth, anti-cheat at scale.
* Browser-playable Mario.
* Uploading arbitrary generator code from users (sandboxing).
* “Continuous generation” (Stage 0 assumes a fixed pool of levels on disk).

---

## Architecture (Stage 0)

Stage 0 is three components with **hard boundaries** so each can be developed independently:

1. **Client (Java game executable)**
2. **Backend (Docker container)**
3. **Database (persistent local storage outside the container)**

### 1) Java Client responsibilities

* Treat the backend as a black box.
* Ask for a battle: “Give me two levels to compare.”
* Run both levels with identical UX structure (so comparisons are fair).
* Collect:

  * **vote**: LEFT/RIGHT/TIE/SKIP
  * optional: completion, deaths, time, subjective tags
* Submit vote to backend and show updated rating snapshot (or at least confirmation).

### 2) Backend responsibilities

* Own all business logic:

  * selecting battles,
  * tracking sessions,
  * validating vote payloads,
  * updating ratings,
  * writing everything to the DB.
* Serve a stable API contract so the Java client doesn’t need to know internals.
* Provide admin/debug endpoints locally (optional but useful).

### 3) Database responsibilities

* Persist:

  * generators and their metadata,
  * levels and attribution,
  * battles served,
  * votes received,
  * rating history (or at least current rating + match count).
* Must survive:

  * backend container restart,
  * backend image rebuild.

---

## Repository layout

### Current structure (Stage 0)

```
pcg-arena/
  README.md
  docker-compose.yml       # ✅ One-command setup (docker compose up)
  .gitignore               # ✅ Excludes db/local/, client-java/dist/

  backend/                 # ✅ Dockerized Python/FastAPI backend
    spec.md                # ✅ Backend implementation documentation
    Dockerfile             # ✅ Container build instructions
    requirements.txt       # ✅ Python dependencies (FastAPI, Uvicorn)
    config/
      settings.env         # ✅ Environment variable defaults
    src/
      __init__.py          # ✅ Package metadata
      config.py            # ✅ Configuration loader
      main.py              # ✅ FastAPI app + endpoints
      db/
        __init__.py        # ✅ Module exports
        connection.py      # ✅ SQLite connection management
        migrations.py      # ✅ Migration runner
        seed.py            # ✅ Seed importer + validation
    tests/                 # ⏳ Placeholder
    openapi/               # ⏳ Placeholder
    scripts/               # ⏳ Placeholder

  db/
    spec.md                # ✅ Database specification
    migrations/            # ✅ SQL migration scripts
      001_init.sql         # ✅ Core tables (7 tables)
      002_indexes.sql      # ✅ Performance indexes
    seed/                  # ✅ Initial dataset
      generators.json      # ✅ 3 generators (hopper, genetic, notch)
      levels/              # ✅ 30 level files (10 per generator)
        genetic/
        hopper/
        notch/
    local/                 # ✅ Runtime storage (gitignored)
      arena.sqlite         # ✅ SQLite database (created on first run)

  client-java/             # ⏳ Not yet implemented
    src/
    assets/
      levels/
    dist/                  # (gitignored)

  shared/                  # ⏳ Placeholder
    schemas/
    level-format/

  docs/
    stage0-spec.md         # ✅ Detailed technical specification
    future-notes.md        # ✅ Future roadmap notes
```

**Legend:**  
✅ Implemented | ⏳ Placeholder / Not yet implemented

### Notes

* `docker-compose.yml` orchestrates backend + volume mounts
* `db/local/` and `client-java/dist/` are gitignored
* Backend serves HTTP API on `localhost:8080`
* Database persists across container restarts

---

## Stage 0 data model (conceptual)

Minimum entities:

* **Generator**

  * `generator_id`, `name`, `version`, `description`, `tags`, `documentation-url`
* **Level**

  * `level_id`, `generator_id`, `content_blob`, `content_format`, `hash`
* **Battle**

  * `battle_id`, `left_level_id`, `right_level_id`, `served_at`, `session_id`
* **Vote**

  * `vote_id`, `battle_id`, `result` (LEFT/RIGHT/TIE/SKIP), `created_at`
  * optional telemetry: `left_tags`, `right_tags`, `telemetry_json`
* **Rating**

  * `generator_id`, `rating_value`, `rating_deviation` (optional), `games_played`

Stage 0 should also store **immutable logs** (battle served → vote received) so you can debug issues and later publish datasets.

---

## Rating system (Stage 0 recommendation)

Because the platform is about **pairwise human preference**, you want a rating that:

* updates online,
* supports uncertainty / cold-start,
* is easy to explain.

### Minimal implementation: Elo (pairwise winners)

* Treat each generator as a “player”.
* Each vote is a “match” between two generators.
* Outcome: win/loss/tie/skip.
* Update Elo after each match.

This gives you the “ELO-like generator leaderboard” you want with extremely low complexity.

---

## Level format (Stage 0)

Stage 0 only needs a format that:

* the Java client can load quickly,
* the backend can treat as opaque bytes + metadata,
* supports a fixed set of tile types.

We will use **ASCII grid / tilemap** (easy, transparent)

It will always be a rectangle of variable width (up to 250) by 16 (LEVEL_HEIGHT) lines, consisting of ASCII characters. E.g. `-` denotes air, `X` ground, `t` pipe or `o` coin.

---

## API contract (Stage 0)

Stage 0 should keep the client/backend integration extremely boring and stable.

### Suggested endpoints

**Fetch a battle**

* `POST /v1/battles:next`
* Request:

  * `client_version`, `session_id` (client-generated UUID), optional `player_id` (anonymous for Stage 0)
* Response:

  * `battle_id`
  * `left`: `{level_id, generator, format, level_payload, ...}`
  * `right`: same structure
  * optional: `instructions`, `allowed_actions` (if needed)

**Submit a vote**

* `POST /v1/votes`
* Body:

  * `battle_id`
  * `result`: `LEFT | RIGHT | TIE | SKIP`
  * optional `left_tags`, `right_tags` (per-level tags)
  * optional `telemetry` object

**Leaderboard**

* `GET /v1/leaderboard`
* Returns sorted generator ratings with match counts (and optional confidence).

### Key invariants (Stage 0)

* A `battle_id` can be voted on **once**.
* Votes must be **idempotent** (client retry shouldn’t double-count).
* Backend must validate that both levels in battle exist and belong to known generators.
* All writes are atomic (vote + rating update in one transaction).

Leaderboard should be displayed on localhost by the backend.

---

## Matchmaking (how battles are chosen)

Stage 0 can be simple but should avoid obvious bias:

**Default strategy**

* Uniformly sample two different generators.
* Uniformly sample one level from each generator’s pool.
* Avoid repeating the exact same pair too often (optional).


---

## Local persistence ✅

**Implemented:** Database persists via bind-mount.

The database file `db/local/arena.sqlite` survives:
- Container restarts (`docker compose down && docker compose up`)
- Container rebuilds (`docker compose up --build`)
- Host system reboots

**How it works:**
- Host directory `./db/local` is mounted to `/data` in container
- Backend writes to `/data/arena.sqlite`
- File remains on host filesystem even when container is removed

**Verify persistence:**
1. Start backend, check leaderboard
2. Stop container: `docker compose down`
3. Restart: `docker compose up`
4. Leaderboard shows same data (generators still at rating 1000.0)

---

## Quickstart (local)

### Prerequisites

* Docker + Docker Compose
* (Optional) Python 3.12+ for local development without Docker
* (Future) Java 17+ for client

### Run backend + database

```bash
# From repository root
docker compose up --build
```

Backend starts on `http://localhost:8080`

### Test the implemented features

**View leaderboard:**
```bash
# Open in browser
http://localhost:8080/

# Or fetch JSON
curl http://localhost:8080/v1/leaderboard
```

**Check health:**
```bash
curl http://localhost:8080/health
```

**Run demo script (10 battles + votes):**
```bash
# Bash (Linux/Mac/Docker)
./backend/scripts/demo.sh

# PowerShell (Windows)
.\backend\scripts\demo.ps1
```

**View logs:**
```bash
docker compose logs backend
```

You should see:
- 3 generators imported
- 30 levels imported
- Database ready message with counts

### Persistence test

```bash
# Stop backend
docker compose down

# Restart
docker compose up

# Leaderboard data persists (check http://localhost:8080/)
```

### Reset database

```bash
# Stop and remove volumes
docker compose down

# Delete database file
rm db/local/arena.sqlite

# Restart (recreates from scratch)
docker compose up --build
```

---

## Current implementation status

### Completed (Stage 0 - Infrastructure)

| Component | Status | Description |
|-----------|--------|-------------|
| **Database** | ✅ Complete | SQLite with 7 tables, migrations, indexes |
| **Migrations** | ✅ Complete | Auto-apply on startup, tracked in DB |
| **Seed import** | ✅ Complete | Generators + levels with validation |
| **Docker setup** | ✅ Complete | docker-compose.yml, volume mounts |
| **Backend skeleton** | ✅ Complete | FastAPI app, config, connection mgmt |
| **Health endpoint** | ✅ Complete | `GET /health` |
| **Leaderboard (JSON)** | ✅ Complete | `GET /v1/leaderboard` |
| **Leaderboard (HTML)** | ✅ Complete | `GET /` - human-readable view |

### In progress / Next steps

| Component | Status | Priority |
|-----------|--------|----------|
| **Battle creation** | ⏳ Not started | High - `POST /v1/battles:next` |
| **Vote submission** | ⏳ Not started | High - `POST /v1/votes` |
| **ELO rating logic** | ⏳ Not started | High - rating delta calculation |
| **Java client** | ⏳ Not started | High - gameplay + vote UI |

### Quick demo

You can already test the backend infrastructure:

```bash
# Start backend + database
docker compose up --build

# View leaderboard
# Open http://localhost:8080/ in browser
```

You should see 3 generators with rating 1000.0 and 30 levels imported.

---

## Stage 0 definition of done (what "finished" means)

A Stage 0 release is “done” when:

* You can run **10 battles end-to-end** without manual DB fiddling.
* Ratings update after every vote and remain consistent after restart.
* Adding a new generator requires **only**:

  * adding generator metadata,
  * dropping a bundle of levels into `db/seed/…`,
  * running a seed/import script.

---

## Relationship to the broader Mario PCG ecosystem

This repo sits at the intersection of:

* **PCG research** (generators, representations, controllability, quality, diversity),
* **human evaluation** (preferences, fun, frustration),
* and the tradition of Mario evaluation frameworks originating from the **Mario AI competitions** and related tooling (notably Java-based environments). 

Long-term, this can complement purely automated metrics (e.g., playability via A* agents) by providing the thing that is often missing: **human preference data at scale**.

## Roadmap (beyond Stage 0)

* **Stage 1 (MVP hosted)**: deploy backend, basic web leaderboard, downloadable client.
* **Stage 2 (research utility)**: better matchmaking, generator versioning, exportable preference dataset.
* **Stage 3 (community platform)**: browser play, accounts, moderation, sandboxed generator submissions.

Stage 0 is intentionally narrow: it exists to prove the loop, not to build a community platform immediately.

---

## License and contribution philosophy

* Stage 0 is built to be open-source friendly: clear API, clear data model, minimal dependencies.
* Contributions later should focus on:

  * new generators (as level bundles in Stage 0),
  * new rating methods (without breaking stored data),
  * UI/UX improvements in the Java client.
